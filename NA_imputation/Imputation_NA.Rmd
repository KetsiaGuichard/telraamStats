# Importation des packages

```{r}

# remove.packages("telraamStats")
# 
# 
# devtools::install_github("https://github.com/KetsiaGuichard/telraamStats",
#                               dependencies = TRUE, 
#                               build_vignettes = TRUE,
#                               force = TRUE)


pacman::p_load(tidyverse,telraamStats,lubridate,forecast)

```

# Importations des besoins de l'environments

```{r}
usethis::edit_r_environ()

Sys.getenv("token")
```

# Création des Fonctions

## Grouper les fonctions par dates ou par saisons

```{r}
#Faire par proportion de missing data
groupby_func <- function(data){ 
  group_all <- data %>%  group_by(hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  group_missing <- data %>% filter(traffic$uptime < 0.5) %>% group_by(hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  group <- merge(group_all, group_missing, by = "hour", all = TRUE) %>% rename(n_missing = n.y, n = n.x)
  group$prop_missing <- group$n_missing / group$n * 100
  return(group)
}

#Discrimination des missing data par saisons 
groupby_season_func <- function(data){ 
  group_all <- data %>%  group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  group_missing <- data %>% filter(traffic$uptime < 0.5) %>% group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  group <- merge(group_all, group_missing, by = c("season","hour"), all = TRUE) %>% rename(n_missing = n.y, n = n.x)
  group$prop_missing <- group$n_missing / group$n * 100
  return(group)

}

```

#Supprimer une peridode de données 
```{r}

#Fonction qui prend en entrée segment_clear, un segment et une periode avec date de debut et date de fin et qui la supprime de segment_clear elle renvoie donc 

delete_segment <- function(data,name, start_date, end_date){
  data <- data %>% filter(!(segment_name == name & date >= start_date & date <= end_date))
  return(data)
}


```
## Filtrer les données abscentes

--\> Enlever les heures ou il fait nuit en fonction de la saisons --\> Enlever les périodes ou les capteurs ne fonctionnait pas pendant une longue periode

```{r}
#Filtrer par saison les heures à conserver avec un seuil de missing data
filtered_hours_data <- function(data, seuil = 0.85){
  
  nrow_start <- nrow(data)
  
  #Transform date column to date format
  data$date <- ymd_hms(data$date)
  
  # Add a column for the season or each month 
  data$season <- ifelse(month(data$date) %in% c(3,4,5), "Spring",
                         ifelse(month(data$date) %in% c(6,7,8), "Summer",
                                ifelse(month(data$date) %in% c(9,10,11), "Autumn", "Winter")))
  
  
  #Group by hour of the day and season for all data and missing data
  group_all <- data %>%  group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  group_missing <- data %>% filter(data$uptime < 0.5) %>% group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  
  #Merge the two dataframes and calculate the proportion of missing data
  group <- merge(group_all, group_missing, by = c("season","hour"), all = TRUE) %>% rename(n_missing = n.y, n = n.x)
  group$prop_missing <- group$n_missing / group$n * 100
  
  
    #Create a first plot of the proportion of missing data per hour of the day per season without filtering
  
    p1 <- group %>% ggplot(aes(x = hour, y = prop_missing, color = season)) + geom_line() + geom_point() + 
          labs(title = "Proportion of missing data per hour \n of the day per season before filtering", x = "Hour of the day", y = "Proportion of missing data")
      
    
  # filter the hours where the proportion of missing data is higher than the threshold
  hour_missing <- group %>% filter(prop_missing > seuil) %>% select(hour, season) %>% unique()
  
  
  #Keep the data where the hour is not in the list of hours with high proportion of missing data and where the uptime 
  data <- data %>% filter(!(hour(date) %in% hour_missing$hour & season %in% hour_missing$season))
  
  
  #Count the number of missing data
  number_missing <- nrow(data %>% filter(data$uptime < 0.5))
  
  #Count the number of rows removed
  nrow_substract <-  nrow_start - nrow(data)
  
  #Create a second plot of the proportion of missing data per hour of the day per season after filtering
  #Group by hour of the day and season for all data and missing data
  group_all_filtered <- data %>%  group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  group_missing_filtered <- data %>% filter(data$uptime < 0.5) %>% group_by(season,hour(date)) %>% summarise(n = n()) %>% rename(hour = `hour(date)`)
  
  
  #Merge the two dataframes and calculate the proportion of missing data
  group_filtered <- merge(group_all_filtered, group_missing_filtered, by = c("season","hour"), all = TRUE) %>% rename(n_missing = n.y, n = n.x)
  group_filtered$prop_missing <- group_filtered$n_missing / group_filtered$n * 100
  
  p2 <- group_filtered %>% ggplot(aes(x = hour, y = prop_missing, color = season)) + geom_line() + geom_point() + 
          labs(title = "Proportion of missing data per hour \n of the day per season after filtering", x = "Hour of the day", y = "Proportion of missing data") + ylim(0,100)
  
  p <- patchwork::wrap_plots(p1,p2)

  
  #Return a list with the filtered data, the plot, the threshold, the number of missing data and the number of rows removed
  list_return <-  list(hour_filter = hour_missing, data = data,plot =  p,threshold = seuil,number_missing = number_missing, nrow_substract = nrow_substract)
  return(list_return)
  
}

substract_successive_NA <- function(data_raw, threshold = 24) {
  # Initialisation d'une nouvelle colonne pour stocker les périodes de données manquantes successives
  data_raw$successive_missing <- 0
  
  
  # Sélectionner les colonnes pertinentes et filtrer les données manquantes
  data_false <- data_raw %>% filter(!uptime_quality) %>% arrange(date)
  data_true <- data_raw %>% filter(uptime_quality) %>% arrange(date)


  
  # Initialiser l'index pour la boucle
  i <- 1
  
  while (i < nrow(data_false)) {
    # Compter le nombre de données manquantes successives
    succ <- 1
    while ((i + succ <= nrow(data_false)) && (difftime(data_false$date[i + succ], data_false$date[i + succ - 1], units = "hours") <= 1)) {
      succ <- succ + 1
    }
    
    # Assigner la valeur de succ à toutes les lignes successives trouvées
    data_false$successive_missing[i:(i + succ - 1)] <- succ
    
    # Avancer l'index de la taille de la période successives trouvée
    i <- i + succ
  }
  
  filtered_data_false <- data_false %>% filter(successive_missing < threshold)
  
  # Fusionner les données manquantes
  data <- rbind(filtered_data_false, data_true) %>% arrange(date)

  
  return(data)
}

#Nettoyage NA


Clean_NA_segments <- function(list_data,threshold_missing = 85, threshold_successive = 24) {

    if (is.list(list_data)){
  #WARNING : the function is not working cause the single data is a list 
    segments_clear <- data.frame()
    for(i in 1:length(list_data)){
        seg <- list_data[i]
        data <- substract_successive_NA(data.frame(seg), threshold_successive)
        segments_clear <- rbind(segments_clear,filtered_hours_data(data, threshold_missing)$data)
    } }
  
  else{
    segments_clear <- substract_successive_NA(data.frame(list_data), threshold_successive)
    segments_clear <- filtered_hours_data(segments_clear, threshold_missing)$data
  }
  
  return(segments_clear)
}

```

## Affichage de la temporalité des données manquantes pour chaque segment

```{r}
#Représenter pour tout les capteurs , la saisonnalité des valeurs manquantes 
plot_NA <- function(data){
  data_long <- data %>% filter(!uptime_quality) %>% select(date,segment_name) %>% mutate(value= as.numeric(substr(segment_name,nchar(segment_name)-1,nchar(segment_name))),segment_name = as.factor(as.character(segment_name)))
  data_long %>% ggplot(aes(x = date, y = value,color=segment_name)) + geom_point()
}
```

## Identifié la nature des données manquante

Données manquantes aléatoires unique : Ces données manquantes sont causées par des arrêts de mesure imprévus, généralement liés à des problèmes techniques comme des déconnexions ou des problèmes Wi-Fi, qui affectent un capteur particulier de manière aléatoire. --\> Type 1

Données manquantes aléatoires multiple : Il s'agit des données qui n'ont pas pu être collectées par un ensemble de capteurs en raison des conditions météorologiques, telles que la pluie ou la neige, de manière aléatoire. Type 2

Données manquantes saisonnières : Ces données manquantes se reproduisent périodiquement aux mêmes moments sur tous les capteurs. Elles peuvent être liées à des phénomènes saisonniers comme le cycle solaire, par exemple. Pour l'instant, nous considérerons ces données comme absentes et ne proposerons pas d'imputation. --\> Type 3

```{r}
NA_Type <- function(segments_clear,threshold = 50){
  #Identifier pour chaque données manquantes sont type 1 ou 2
  summarise_NA <- segments_clear %>% group_by(date) %>% summarise(n_missing = sum(uptime < 0.5)) %>% arrange(date)
  summarise_NA$prop_missing <- summarise_NA$n_missing / length(unique(segments_clear$segment_name)) * 100
  
  #Si plus de 50% des données sont manquantes, on considère que la donnée est de type 2 sinon de type 1 
  summarise_NA$type <- ifelse(summarise_NA$prop_missing > threshold, 2, 1)
  
  #Pour chaque date de data_clear on va ajouter le type 
  segments_clear <- left_join(segments_clear, summarise_NA %>% select(date, type), by = "date")
  segments_clear$type[which(segments_clear$uptime_quality == T)] <- NA
  return(segments_clear)
}


```


```{r}
prop.table(table(segments_clear$type))*100 
#Differencier la table des proportions par segment


View(segments_clear %>% group_by(as.character(segment_name)) %>% summarise(n_type1 = sum(type == 1, na.rm = T), n_type2 = sum(type == 2, na.rm = T)) %>% mutate(prop_type1 = round(n_type1 / (n_type1 + n_type2) * 100,2), prop_type2 = round(n_type2 / (n_type1 + n_type2) * 100,2) ))
```


```{r}
#Determiner les. types 3 

#Idée 1: Pour chaque capteur, recuperer les dates ou il y a eu des données manquantes, faire une matrice binaire avec les dates en lignes et les capteurs en colonnes 1 si il y a une valeur manquante 0 sinon

data_type3 <- segments_clear  %>% select(date, segment_name,uptime_quality) %>% mutate(segment_name = as.factor(as.character(segment_name))) 
captor_type3 <- data_type3 %>% spread(key = segment_name, value = uptime_quality) 
captor_type3[is.na(captor_type3)] <- TRUE

date_type3 <- data_type3 %>% spread(key = date, value = uptime_quality) 
date_type3[is.na(date_type3)] <- TRUE

#Corrélation entre les dates 
correlation_captor <- cor(data_type3[,-1], method = "spearman") 
correlation_date <-  cor(date_type3[,-1], method = "spearman")

#CAH sur la matrice date_type3
d <- dist(captor_type3[,-1], method = "euclidean")
hc <- hclust(d, method = "ward.D2")
#Perte d'inertie

```

```{r}
#Determiner les. types 3 

#Idée 2: Regarder la saisonnalité des valeurs manquantes pour chaque capteur puis regarder pour chaque valeurs manquante si elle appartient a une saisonnalité ou non

#On essaie pour un capteur 
data_burel <- segments_clear  %>% filter(segment_name == "Burel-01")

#graphique de la saisonnalité avec ggtsdisplay
library(forecast)
ggtsdisplay(as.numeric(data_burel$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur Burel-01")


burel_weekday <- segments_clear  %>% filter(segment_name == "Burel-01",!weekday %in% c("saturday","sunday"))
burel_weekend <- segments_clear  %>% filter(segment_name == "Burel-01",weekday %in% c("saturday","sunday"))
burel_holiday <- segments_clear  %>% filter(segment_name == "Burel-01",holiday)
burel_notHoliday <- segments_clear  %>% filter(segment_name == "Burel-01",!holiday)

ggtsdisplay(as.numeric(burel_weekday$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur Burel-01 en semaine")
ggtsdisplay(as.numeric(burel_weekend$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur Burel-01 en week-end")
ggtsdisplay(as.numeric(burel_holiday$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur Burel-01 en jour férié")
ggtsdisplay(as.numeric(burel_notHoliday$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur Burel-01 en jour ouvré")


```

# Importation et nettoyage des données

```{r}
load("/Users/paulvallee/Desktop/Stage CREM/Code R/data/segments_2ans.RData")
data_04$segment_name <- "RueDesEcoles-14"
segments <- list(data_01,data_02,data_04,data_05,data_06,data_07,data_08,data_10,data_11,data_13,data_14,data_15,data_16,data_18)
segments_clear <- Clean_NA_segments(segments, 85,24)

segments_clear$segment_name <- as.factor(as.character(segments_clear$segment_name))
segments_clear$weekday <- as.factor(as.character(segments_clear$weekday))
segments_clear$holiday <- as.factor(as.character(segments_clear$holiday))
segments_clear$date <- as.Date(segments_clear$date)
```

```{r}
V2 <- c("Burel-01","ParisArcEnCiel-05","RueGdDomaine-07", "RuePrieure-11")
V1 <- c("Leclerc-02","rueVignes-04", "RteVitre-06", "StDidierSud-10","RueVeronniere-13", "RueDesEcoles-14", "RueManoirs-15", "RueToursCarree-16", "BoulevardLiberté-18")
KO <- c("ParisMarche-03","StDidierNord-08","RueGdDomaine-09","RueVallee","RueCottage-12","PlaceHotelDeVille-17")

segments_clear$version <- ifelse(segments_clear$segment_name %in% V2, "V2", ifelse(segments_clear$segment_name %in% V1, "V1", ifelse(segments_clear$segment_name %in% KO, "KO", NA)))

```


## Données météo

*T        : température
UV       : rayonnement ultra-violet horaire en heure UTC (en J/cm2)
DIR2     : rayonnement direct horaire en heure TSV (en J/cm2)
*INS     : insolation horaire en heure UTC (en mn)
VV       : visibilité (en m)
UX       : humidité relative maximale dans l’heure (en %)
RR1      : quantité de précipitation tombée en 1 heure (en mm et 1/10 mm)
DRR1     : durée des précipitations (en mn)

```{r}
meteo <- read.csv("/Users/paulvallee/Desktop/Stage CREM/Code R/data/meto_St_jac_05_2023_2024.csv", sep = ";")
meteo <- meteo %>% select(DATE,`T`,UV,DIR2,INS,VV,UX,RR1,DRR1)
meteo$DATE <- ymd_h(meteo$DATE)
meteo$DATE <- with_tz(meteo$DATE, tzone =)
```

```{r}
segments_clear <- segments_clear %>% left_join(meteo, by = c("date" = "DATE"))

segments_clear
```




# Analyse des données
```{r}
plot_NA(segments_clear %>% filter(type== 1))
plot_NA(segments_clear %>% filter(type== 2))
plot_NA(segments_clear)
#Le graphique représente les dates ou les données ne sont pas manquantes Le titre du graphique peut etre :  "Dates ou les données sont manquantes pour le capteur RueDesEcoles-14"
```
```{r}

t <- segments_clear %>%  filter(segment_name == "RueToursCarree-16") 
plot.ts(t$uptime)

ggtsdisplay(as.numeric(t$uptime),main = "Saisonnalité des valeurs manquantes pour le capteur RueToursCarree-16")

#lag de 10
ggtsdisplay(as.numeric(diff(t$uptime, lag = 10,differences = 1)
),main = "Saisonnalité des valeurs manquantes pour le capteur RueToursCarree-16")

```

```{r}
#Comparer la tendance de VV avec uptime
t <- segments_clear %>%  filter(segment_name == "RueToursCarree-16")
plot.ts(t$VV)
plot.ts(t$uptime)



```




## Corrélation

```{r}
segment_cor <- segments_clear %>% mutate(segment_name = as.factor(as.character(segment_name)),heavy = as.numeric(heavy),car = as.numeric(car)) %>%  select(date,segment_name,heavy,car) %>% mutate(vehicle = as.numeric(heavy + car)) %>% select(-heavy,-car) %>% pivot_wider(names_from = segment_name,values_from = vehicle) 


d <- segment_cor %>% select(-date,-`rueVignes-04`,`StDidierSud-10`)
na.omit(d) %>% cor() %>% corrplot::corrplot( method = "number", type = "upper", tl.col = "black", tl.srt = 45)


```




# Imputation des données manquantes

## Plus proche voisin par proximité géographique entre les capteurs

conclusion : peu concluant

```{r}
library(igraph)

# Créer un dataframe avec les relations entre les segments
relations <- data.frame(
  segment_name = c("Burel-01", "Leclerc-02", "rueVignes-04", "ParisArcEnCiel-05", "RteVitre-06", "RueGdDomaine-07", "StDidierSud-10", "RuePrieure-11", "RueDesEcoles-14", "RueManoirs-15", "RueVeronniere-13", "RueToursCarree-16", "PlaceHotelDeVille-17", "BoulevardLiberte-18", "BoulevardLaennecSud", "BoulevardLaennecNord"),
  destination = c("BoulevardLaennecNord", "RueManoirs-15", "RuePrieure-11", "ParisMarche-03", "RueGdDomaine-07", "RteVitre-06", "RueDesEcoles-14", "rueVignes-04", "StDidierSud-10", "ParisArcEnCiel-05", "ParisArcEnCiel-05", "RuePrieure-11", "RuePrieure-11", "BoulevardLaennecSud", "BoulevardLaennecNord", "BoulevardLaennecSud")
)

# Convertir segment_name et destination en facteur
relations$segment_name <- as.factor(relations$segment_name)
relations$destination <- as.factor(relations$destination)

# Créer le graphe à partir du dataframe
graph <- graph_from_data_frame(relations, directed = TRUE)

# Afficher le graphe
plot(graph, layout = layout.auto)


#Création de la matrice d'adjacence
adjacency_matrix <- as_adjacency_matrix(graph, attr = NULL, edges = FALSE, names = TRUE, sparse = FALSE)
adjacency_matrix
```

```{r}
set.seed(123)
#Recuperer la sous matrice correspondant au colonne de test
adjacency_matrix_sub <- adjacency_matrix[1:9,1:9]
colnames(adjacency_matrix_sub)

temoin <- segments_clear %>% filter(uptime > 0.5) %>% select(date,segment_name,heavy,car) %>% mutate(vehicle = heavy + car) %>% select(-heavy,-car) %>% pivot_wider(names_from = segment_name,values_from = vehicle)  %>% na.omit()

#Crée des valeurs manquantes aléatoire dans le jeu de données
test2 <- temoin
# Nombre de valeurs manquantes à créer
num_missing <- 50

vec_x <- c()
vec_y <- c()

for(i in 1:num_missing){
  # Choisir un élément aléatoire dans le jeu de données
  x <- sample(1:nrow(test), 1)
  y <- sample(2:ncol(test), 1)
  vec_x <- c(vec_x,x)
  vec_y <- c(vec_y,y)
  # Remplacer la valeur correspondante par NA
  test2[x,y] <- NA
}

test2
```

```{r}
# Reconstruire test2 en utilisant les valeurs des capteurs voisins
for (j in 2:ncol(test2)){
  for (i in 1:nrow(test2)){
    if(is.na(test2[i,j])){
      # Recuperer nom colonne
      col_name <- colnames(test2)[j]
      # Recuperer les noms des colonnes voisines
      neighbors <- colnames(adjacency_matrix_sub)[which(adjacency_matrix_sub[col_name,] == 1)]
      
      #Si il existe un voisin et que sa valeur est non manquante alors remplacé par cette valeurs 
      if(length(neighbors) == 1 ){
            if(!is.na(test2[i,neighbors])){
              test2[i,j] <- test2[i,neighbors]
            }
      }
      else if(length(neighbors) > 1){
        #Si il existe plusieurs voisins alors on prend la moyenne
        test2[i,j] <- mean(test2[i,neighbors],na.rm = TRUE)
        
      }
      else if(length(neighbors) == 0){
        #Si il n'existe pas de voisin alors on prend la moyenne de la colonne
        test2[i,j] <- NA}
    }
  }
}
      
is.na(test2) %>% sum()


mean_error <- 0
#Comparer les valeurs imputé avec les valeurs réels
for(i in 1:length(vec_x)){
    print(paste("Valeur réel : ",test[vec_x[i],vec_y[i]]," Valeur imputé : ",test2[vec_x[i],vec_y[i]]))
  }

```


# Test des algorithmes de clustering pour l'imputation des données

Quel Stratégie : A partir des groupes formé par le clustering on va imputer les valeurs manquantes.
  --> Moyenne des voisins ?
  --> Prendre en compte les valeurs du capteur a des dates similaires ? 
  



## Algorithme Kmeans

Avantage permet de prendre en compte la nature des routes 

```{r}
#Pour les differents capteurs, effectuer un kmeans pour trouver des groupes de capteurs similaires 

#On utilise la distance euclidienne pour mesurer la similarité entre les capteurs
dkmeans1 <- segments_clear %>%  mutate(segment_name = as.factor(as.character(segment_name))) %>% 
  select(segment_name, heavy_rgt, heavy_lft, car_rgt, car_lft, bike_rgt, bike_lft, pedestrian_lft, pedestrian_rgt,v85)  %>%
  na.omit() %>% 
  group_by(segment_name) %>% 
  summarise(mean_V85 = mean(v85),
            mean_heavy_rgt = mean(heavy_rgt),
            mean_heavy_lft = mean(heavy_lft),
            mean_car_rgt = mean(car_rgt),
            mean_car_lft = mean(car_lft),
            mean_bike_rgt = mean(bike_rgt),
            mean_bike_lft = mean(bike_lft),
            mean_pedestrian_lft = mean(pedestrian_lft),
            mean_pedestrian_rgt = mean(pedestrian_rgt)) 


dkmeans2 <- segments_clear %>%  mutate(segment_name = as.factor(as.character(segment_name)),vehicle = car + heavy) %>% 
  select(segment_name, vehicle,v85) %>%  na.omit() %>% group_by(segment_name)  %>% summarise(mean_vehicle = mean(vehicle),mean_V85 = mean(v85)) 


#On effectue un kmeans sur les données pour k allant de 2 à 5
kmeans1 <- dkmeans1 %>% select(-segment_name) %>% scale() %>% kmeans(centers = 4)
kmeans2 <- dkmeans2 %>% select(-segment_name) %>% scale() %>%  kmeans(centers = 4)

#On représente graphiquement les groupes de capteurs
dkmeans1$cluster <- kmeans1$cluster
dkmeans2$cluster <- kmeans2$cluster

#Ajouter le nom des points 
dkmeans1 %>% ggplot(aes(x = mean_V85, y = mean_heavy_rgt, color = as.factor(cluster))) + geom_point() + ggtitle("Kmeans sur les capteurs") + xlab("V85") + ylab("Heavy_rgt") + geom_text(aes(label = segment_name),hjust = 0, vjust = 0)

dkmeans2 %>% ggplot(aes(x = mean_V85, y = mean_vehicle, color = as.factor(cluster))) + geom_point() + ggtitle("Kmeans sur les capteurs") + xlab("V85") + ylab("Vehicle") + geom_text(aes(label = segment_name),hjust = 0, vjust = 0)

```



```{r}
#Prendre en compte des variables catégorielles 
data_kmeans <- data.frame()
for(var_quali in c("vacation","weekday","season")){
  df <- segments_clear %>%  mutate(segment_name = as.factor(as.character(segment_name))) %>% 
    na.omit() %>% 
    group_by(segment_name,.data[[var_quali]]) %>% 
    summarise(mean_V85 = mean(v85),
              mean_heavy_rgt = mean(heavy_rgt),
              mean_heavy_lft = mean(heavy_lft),
              mean_car_rgt = mean(car_rgt),
              mean_car_lft = mean(car_lft),
              mean_bike_rgt = mean(bike_rgt),
              mean_bike_lft = mean(bike_lft),
              mean_pedestrian_lft = mean(pedestrian_lft),
              mean_pedestrian_rgt = mean(pedestrian_rgt)) 
    
    data_kmeans <- rbind(data_kmeans,df)
}

data_kmeans <- data_kmeans %>%
  mutate(
    segment_name = case_when(
      !is.na(vacation) ~ paste0(segment_name, "_", as.character(vacation)),
      !is.na(weekday) ~ paste0(segment_name, "_", as.character(weekday)),
      !is.na(season) ~ paste0(segment_name, "_", as.character(season)),
      TRUE ~ segment_name
    )
  ) %>% select(-vacation,-weekday,-season)

kmeans3 <- data_kmeans  %>% data.frame() %>% select(-segment_name) %>% scale() %>% kmeans(centers = 4) 

data_kmeans$cluster <- kmeans3$cluster

data_kmeans %>% ggplot(aes(x = mean_V85, y = mean_heavy_rgt, color = as.factor(cluster))) + geom_point() + ggtitle("Kmeans sur les capteurs") + xlab("V85") + ylab("Heavy_rgt") + geom_text(aes(label = segment_name),hjust = 0, vjust = 0)


```


# Création de la pipeline de test 
```{r}
#On crée une pipeline pour tester les différentes stratégies d'imputation

Create_test_data <- function(data,num_missing=100,seed = NA){

#Creer la seed si spécifié 
if(!is.na(seed)){set.seed(seed)}
  
data <- data %>% na.omit() %>% filter(uptime_quality) %>% mutate(vehicle = car + heavy)


#Dans un premier temps on peut créer nos valeurs manquante 
temoin <- data
test <- data

vec_sample <- sample(1:nrow(data),num_missing)
test$vehicle[vec_sample] <- NA


list(temoin=temoin,test=test)
}

Create_test_data(segments_clear,num_missing = 1000)$test
```


# Imputation par serie temporelle
```{r}
library(prophet)

data_proph <- segments_clear %>% filter(segment_name == "Burel-01") %>% select(date,uptime) %>% mutate(ds = date, y = uptime)


data_proh <- data_01 %>% select(date,uptime) %>% mutate(ds = date, y = uptime)

#Prendre en compte week end et vacances
data_proph <- segments_clear %>% filter(segment_name == "Burel-01") %>% select(date,uptime,vacation,weekday) %>% mutate(ds = date, y = uptime)

m <- prophet(data_proph)
pred <- predict(m, data_proph)
plot(m, pred)
prophet_plot_components(m, pred)
dyplot.prophet(m, pred)


```


```{r}
#Prendre en compte week end et vacances
data_proph <- segments_clear  %>%filter(segment_name == "ParisArcEnCiel-05") %>% mutate(ds = date, y = uptime)  %>% select(ds,y,vacation,weekday)

m <- prophet(data_proph,holidays=vacation,weekday)
pred <- predict(m, data_proph)
plot(m, pred)
prophet_plot_components(m, pred)
#dyplot.prophet(m, pred)



```
```{r}
data_proph <- segments_clear  %>%filter(segment_name == "ParisArcEnCiel-05") %>% mutate(vehicle = ifelse(uptime_quality,car + heavy,NA)) %>%  mutate(ds = date, y = vehicle) %>%  select(ds,y,weekday)

holidays <- data_proph %>% filter(vacation != "No vacation") %>% mutate(lower_window = 0, upper_window = 1, vacation = as.character(vacation)) %>% rename(holiday = vacation) %>%
select(holiday,ds,lower_window,upper_window)

m <- prophet(data_proph,holidays=holidays)
pred <- predict(m, data_proph)
plot(m, pred)
prophet_plot_components(m, pred)
#dyplot.prophet(m, pred)
```


```{r}
pred_date<- segments_clear  %>%filter(segment_name == "ParisArcEnCiel-05",!uptime_quality) %>% mutate(ds = date) %>% select(ds)

pred <- predict(m, pred_date)
pred

#Afficher les prédictions sur un graphique
dpred <- pred  %>% select(ds,yhat) %>% rename(y = yhat) %>% mutate(type = "pred",y = ifelse(y < 0,0,y)) 
dtrain <- data_proph %>% na.omit() %>% select(ds,y) %>% mutate(type = "real")

dtrain$ds <- as.POSIXct(dtrain$ds)
dpred$ds <- as.POSIXct(dpred$ds)

#supperposer les graphique de dpred et dtrain 
ggplot() + 
  geom_point(data = dtrain, aes(x = ds, y = y, color = type)) + 
  geom_point(data = dpred, aes(x = ds, y = y, color = type)) + 
  ggtitle("Prédiction de la fréquentation du segment ParisArcEnCiel-05") + 
  xlab("Date") + 
  ylab("Nombre de véhicules") + 
  theme(legend.position = "none")
```



```{r}
TimeSeries_Imputation <- function(data){
  library(prophet)
  

  train_set <- data %>% mutate(ds = date, y =ifelse(uptime_quality,car + heavy,NA)) %>% select(ds,y)
  test_set <- data %>% filter(!uptime_quality) %>% mutate(ds = date) %>% select(ds)
  
  holidays <- data %>% filter(holiday) %>% mutate(lower_window = 0, upper_window = 1, vacation = as.character(vacation)) %>% 
    select(vacation,date,lower_window,upper_window) %>%  rename(holiday = vacation,ds = date)
  
  
  model <- prophet(train_set,holidays=holidays)
  pred <- predict(model, test_set)
  
  ajust <-  predict(model, train_set)
  fitting_curve <- plot(model, ajust)
  
  components <- prophet_plot_components(model, pred)
  
  dpred <- pred  %>% select(ds,yhat) %>% rename(y = yhat) %>% mutate(type = "pred",y = ifelse(y < 0,0,y))
  dtrain <- train_set %>% na.omit() %>% select(ds,y) %>% mutate(type = "real")
  
  dtrain$ds <- as.POSIXct(dtrain$ds)
  dpred$ds <- as.POSIXct(dpred$ds)
  
  plot_inputdata <- ggplot() +
    geom_point(data = dtrain, aes(x = ds, y = y, color = type)) +
    geom_point(data = dpred, aes(x = ds, y = y, color = type)) +
    ggtitle(paste("Prédiction de la fréquentation du segment",data$segment_name[1] )) +
    xlab("Date") +
    ylab("Nombre de véhicules") +
    theme(legend.position = "none")
  
  
  return(list(prediction = dpred ,fitting_curve = fitting_curve, components = components, Imputation = plot_inputdata))
  
  
}

```


```{r}
Leclerc2 <- segments_clear %>% filter(segment_name == "Leclerc-02")
obj_input <- TimeSeries_Imputation(Leclerc2)
obj_input$components
obj_input$fitting_curve
obj_input$Imputation
```


```{r}
RueManoir5 <- segments_clear %>% filter(segment_name == "RueManoirs-15")
obj_input <- TimeSeries_Imputation(RueManoir5)
obj_input$components
obj_input$fitting_curve
obj_input$Imputation
```


```{r}
ParisAEC5 <- segments_clear %>% filter(segment_name ==  "ParisArcEnCiel-05") 
obj_input <- TimeSeries_Imputation(ParisAEC5)
obj_input$components
obj_input$fitting_curve
obj_input$Imputation
```
```{r}
BDLiberte <- segments_clear %>% filter(segment_name ==  "Burel-01")
obj_input <- TimeSeries_Imputation(BDLiberte)
obj_input$components
obj_input$fitting_curve
obj_input$Imputation
```

```{r}
Clean_NA_segments_list <- function(list_data,threshold_missing = 85, threshold_successive = 24) {
  list_clear_data <- list()
   for(i in 1:length(list_data)){
        seg <- list_data[i]
        data <- substract_successive_NA(data.frame(seg), threshold_successive)
        
        data_name = as.character(seg$segment_name[1])
        list_clear_data[[i]] <- filtered_hours_data(data, threshold_missing)
   } 
  
  return(list_clear_data)
}

liste_seg <- Clean_NA_segments_list(segments)


TimeSeries_Imputation_list <- function(list_data){
 
   library(prophet)
  list_pred <- list()
  for(i in 1:length(list_data)){
        data <- list_data[[i]]$data
    
    train_set <- data %>% mutate(ds = date, y =ifelse(uptime_quality,car + heavy,NA)) %>% select(ds,y)
    test_set <- data %>% filter(!uptime_quality) %>% mutate(ds = date) %>% select(ds)
    
    holidays <- data %>% filter(holiday) %>% mutate(lower_window = 0, upper_window = 1, vacation = as.character(vacation)) %>% 
      select(vacation,date,lower_window,upper_window) %>%  rename(holiday = vacation,ds = date)
    
    
    model <- prophet(train_set,holidays=holidays)
    pred <- predict(model, test_set)
    
    ajust <-  predict(model, train_set)
    fitting_curve <- plot(model, ajust)
    
    components <- prophet_plot_components(model, pred)
    
    dpred <- pred  %>% select(ds,yhat) %>% rename(y = yhat) %>% mutate(type = "pred",y = ifelse(y < 0,0,y))
    dtrain <- train_set %>% na.omit() %>% select(ds,y) %>% mutate(type = "real")
    
    dtrain$ds <- as.POSIXct(dtrain$ds)
    dpred$ds <- as.POSIXct(dpred$ds)
    
    plot_inputdata <- ggplot() +
      geom_point(data = dtrain, aes(x = ds, y = y, color = type)) +
      geom_point(data = dpred, aes(x = ds, y = y, color = type)) +
      ggtitle(paste("Prédiction de la fréquentation du segment",data$segment_name[1] )) +
      xlab("Date") +
      ylab("Nombre de véhicules") +
      theme(legend.position = "none")
  
   output =  list(prediction = dpred ,fitting_curve = fitting_curve, components = components, Imputation = plot_inputdata,model=model)
   list_pred[[i]] <- output
  }
  
  return(list_pred)
  
}

liste_pred <- TimeSeries_Imputation_list(liste_seg)

liste_seg[[12]]$data <- delete_segment(liste_seg[[12]]$data,"RueManoirs-15","2023-10-01","2024-01-01")
liste_pred[[12]] <- TimeSeries_Imputation(liste_seg[[12]]$data)

liste_pred[[12]] <- TimeSeries_Imputation(liste_seg[[12]]$data)


```

```{r}
#sur une seul fenetre
par(mfrow=c(2,2))
liste_pred[[1]]$components
```

```{r}
for(i in 1:length(liste_pred)){
  print(liste_pred[[i]]$fitting_curve)
  print(liste_pred[[i]]$Imputation)
}
```



Pour mesurer la qualité d'un modèle temporel, vous pouvez utiliser plusieurs tests statistiques, en fonction de vos besoins et du type de modèle que vous avez utilisé. Voici quelques options courantes :

Erreur moyenne absolue (MAE) : Mesure la moyenne des écarts absolus entre les valeurs prédites et les valeurs réelles. Plus la MAE est faible, meilleure est la performance du modèle.

Erreur quadratique moyenne (MSE) : Calcule la moyenne des carrés des écarts entre les valeurs prédites et les valeurs réelles. Comme pour la MAE, une valeur plus faible indique une meilleure performance, mais elle donne plus de poids aux grandes erreurs.

Erreur quadratique moyenne racine (RMSE) : C'est la racine carrée de la MSE. Comme la MSE, mais exprimée dans les mêmes unités que la variable cible, ce qui la rend plus interprétable.

Erreur absolue moyenne en pourcentage (MAPE) : Calcule la moyenne des écarts absolus en pourcentage entre les valeurs prédites et les valeurs réelles. Utile pour évaluer la précision relative du modèle.

Coefficient de détermination (R²) : Indique la proportion de la variance de la variable dépendante qui est prévisible à partir de la variable indépendante. Une valeur proche de 1 indique un bon ajustement du modèle.

Test de Ljung-Box : Test de l'autocorrélation des résidus d'un modèle ARIMA. Il évalue si les résidus sont indépendants les uns des autres.

Test de Durbin-Watson : Utilisé pour tester l'autocorrélation des résidus dans un modèle de régression. Il teste si les résidus présentent une corrélation linéaire les uns avec les autres.

Critère d'information d'Akaike (AIC) et Critère d'information bayésien (BIC) : Mesurent la qualité d'ajustement d'un modèle tout en pénalisant la complexité. Des valeurs plus basses indiquent un meilleur ajustement du modèle.


```{r}
#Tester la qualité du modeles 
model <- liste_pred[[1]]$model

#MAE
MAE <- mean(abs(model$yhat - model$y))
MAE

#MSE
MSE <- mean((model$fitted - model$y)^2)
MSE
#RMSE
RMSE <- sqrt(MSE)
RMSE
#MAPE
MAPE <- mean(abs((model$fitted - model$y)/model$y))
MAPE
#R²
R2 <- 1 - sum((model$y - model$fitted)^2) / sum((model$y - mean(model$y))^2)
R2
#Test de Ljung-Box
Box.test(model$residuals, lag = 10, type = "Ljung-Box")

#Test de Durbin-Watson
durbinWatsonTest(model$residuals)

#AIC
AIC(model)

#BIC
BIC(model)




```




```{r}
dataV2 <- read.csv("/Users/paulvallee/Desktop/Stage CREM/Code R/data/20230518_20240517_v2_sensors_extract.csv")
dataV2$uptime_quality <- ifelse(dataV2$uptime >=0.5 ,TRUE,FALSE)
clean_dataV2 <- Clean_NA_segments(dataV2)


```


